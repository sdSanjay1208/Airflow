# ğŸ“˜ Enterprise Data Cleaning & ETL Orchestration Framework using Python

## ğŸ“Œ Project Overview

This project implements an **Enterprise-level ETL (Extract, Transform, Load) framework** using **Apache Airflow, Python, MySQL, and Docker**.

The system is designed to:
- Ingest large datasets
- Clean and standardize data
- Apply transformations
- Manage historical changes
- Monitor execution using metadata and dashboards

The pipeline follows a **layered data architecture**:

- **RAW Layer** â€“ Stores ingested source data  
- **STAGING Layer** â€“ Cleansed and standardized data  
- **CURATED Layer** â€“ Business-ready, deduplicated, and validated data  

The framework is fully automated using **Airflow DAGs** and optimized for **large-scale data processing**.

---

## ğŸ¯ Objectives

- Build a scalable ETL pipeline using Apache Airflow  
- Implement data cleaning and standardization techniques  
- Handle large datasets efficiently  
- Maintain data quality and validation checks  
- Track pipeline execution using metadata logging  
- Optimize performance using parallelism and batch processing  
- Provide visibility through dashboards and logs  

---

## ğŸ—ï¸ System Architecture
```text
Source CSV / API
        â†“
     RAW Layer
        â†“
   STAGING Layer
        â†“
   CURATED Layer
        â†“
 Validation & Metadata
        â†“
 Dashboard & Reports
```
* Airflow DAGs orchestrate the entire pipeline
* MySQL stores data and metadata
* Docker ensures consistent environment setup
* Python handles transformation logic

## ğŸ› ï¸ Tech Stack

* Apache Airflow 2.8+
* Python 3.8+
* MySQL
* Docker & Docker Compose
* Streamlit (Dashboard)
* Pandas & SQL
* Git & GitHub

# ğŸš€ Sprint-wise Implementation Summary
### Sprint 1 â€“ Environment Setup & Pipeline Design

* Airflow installation using Docker
* DAG structure design
* Database schema creation (RAW, STAGING, CURATED)

### Sprint 2 â€“ Data Cleaning & SCD Implementation

* Lookup table creation
* Standardization (uppercase/lowercase)
* Fuzzy matching for category mapping
* Slowly Changing Dimension (Type-2) logic
### Sprint 3 â€“ SQL Transformations

* SQL-based transformations
* Validation of row counts
* Modular transformation scripts
### Sprint 4 â€“ API Ingestion

* External API extraction
* Pagination and retry handling
* JSON storage in RAW layer

### Sprint 5 â€“ Metadata & Auditing

* DAG run logging
* Task-level statistics
* Pipeline history tracking
* Metadata query APIs

### Sprint 6 â€“ Version Control

* Dataset versioning
* Script and schema versioning
* Rollback strategy

### Sprint 7 â€“ Dashboard & Monitoring

* Streamlit dashboard
* Execution history view
* Data quality score visualization
* Trend analysis charts

### Sprint 8 â€“ Performance Optimization

* Airflow parallelism tuning
* DAG scheduling optimization
* Batch processing for large datasets
* Database query tuning
* I/O optimization

## ğŸ“ Project Structure

```

Airflow/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yaml          # Docker setup for Airflow, MySQL, Redis
â”‚   â””â”€â”€ .env                         # Environment variables
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ amazon_etl_pipeline.py       # Main CSV-based ETL pipeline DAG
â”‚   â”œâ”€â”€ amazon_sql_scheduler_dag.py  # SQL-based transformation scheduler DAG
â”‚   â””â”€â”€ api_pipeline_dag.py          # External API ingestion DAG
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py                  # Marks scripts as a Python package
â”‚   â”œâ”€â”€ csv_to_raw.py                # CSV â†’ RAW ingestion
â”‚   â”œâ”€â”€ raw_to_staging.py            # RAW â†’ STAGING transformation
â”‚   â”œâ”€â”€ staging_to_curated.py        # STAGING â†’ CURATED transformation
â”‚   â”œâ”€â”€ validate_pipeline.py         # End-to-end pipeline validation
â”‚   â”œâ”€â”€ data_quality_checks.py       # Column-level data quality checks
â”‚   â”œâ”€â”€ scd_category.py              # SCD Type-2 logic for dimensions
â”‚   â”œâ”€â”€ fuzzy_mapper.py              # Fuzzy matching for category standardization
â”‚   â”œâ”€â”€ metadata_logger.py           # Logs DAG & task metadata
â”‚   â”œâ”€â”€ metadata_api.py              # APIs to expose pipeline metadata
â”‚   â”œâ”€â”€ api_ingestion.py             # Extract data from external APIs
â”‚   â”œâ”€â”€ api_clean_transform.py       # Clean & normalize API data
â”‚   â”œâ”€â”€ api_load_to_mysql.py         # Load API data into MySQL
â”‚   â”œâ”€â”€ run_sql_file.py              # Execute versioned SQL scripts
â”‚   â”œâ”€â”€ schema_versioning.py         # Schema version control
â”‚   â”œâ”€â”€ transform_versioning.py      # Transformation version tracking
â”‚   â””â”€â”€ rollback_transform.py        # Rollback mechanism for failed transforms
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py                       # Streamlit dashboard entry point
â”‚   â”œâ”€â”€ api_client.py                # API client for dashboard
â”‚   â””â”€â”€ charts.py                   # Reusable chart components
â”‚
â”œâ”€â”€ data_models/
â”‚   â”œâ”€â”€ raw_schema.sql               # RAW layer table definitions
â”‚   â”œâ”€â”€ staging_schema.sql           # STAGING layer table definitions
â”‚   â”œâ”€â”€ curated_schema.sql           # CURATED layer table definitions
â”‚   â”œâ”€â”€ lookup_tables.sql            # Lookup & reference tables
â”‚   â””â”€â”€ metadata_tables.sql          # Metadata & audit tables
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                         # Raw source files (CSV / API dumps)
â”‚   â”œâ”€â”€ staging/                     # Intermediate transformed data
â”‚   â””â”€â”€ processed/                   # Final processed datasets
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ airflow/                     # Airflow execution logs
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ETL Documentation
â”‚
â”œâ”€â”€ README.md                        # Project documentation
â”œâ”€â”€ LICENSE                          # MIT License
â””â”€â”€ requirements.txt                 # Python dependencies
```
## ğŸš€ Setup & Installation

### 1ï¸âƒ£ Prerequisites

Make sure the following are installed:
- Docker
- Docker Compose
- Git
---

### 2ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/Enterprise_ETL_Framework.git
cd Enterprise_ETL_Framework
```
### 3ï¸âƒ£ Configure Environment Variables

Create a `.env` file in the project root directory:

```env
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

AIRFLOW_UID=50000
AIRFLOW__CORE__FERNET_KEY=your_fernet_key
AIRFLOW__WEBSERVER__SECRET_KEY=your_secret_key
```
## 4ï¸âƒ£ Start Airflow Using Docker

```bash
docker compose up -d --build
```
### 5ï¸âƒ£ Access Airflow UI
Open your browser and navigate to:
```bash
http://localhost:8080
```
---
* Default Credentials
* Username: airflow
* Password: airflow

### 6ï¸âƒ£ Trigger the ETL Pipeline

* Open the DAGs page in the Airflow UI
* Enable the DAG amazon_etl_pipeline
* Trigger the DAG manually
### 7ï¸âƒ£ Access the Dashboard
Run the Streamlit dashboard using:
```bash
docker compose exec webserver streamlit run /opt/airflow/dashboard/app.py

Then open:
http://localhost:8501
```
## ğŸ“ŠKey Features

* âœ… Layered architecture (RAW â†’ STAGING â†’ CURATED)
* âœ… Incremental & duplicate-safe ingestion
* âœ… SCD Type-2 implementation
* âœ… Metadata logging (DAG & task level)
* âœ… Dataset, script, and schema version control
* âœ… Data quality validation
* âœ… Dashboard with trends and DQ metrics
* âœ… Performance tuning for large datasets

## ğŸ§ªValidation & Quality Checks

* Row count consistency across layers
* Null checks on critical columns
* Duplicate detection
* DAG execution status validation

## âš™ï¸ Performance Optimization

* Airflow parallelism tuning
* Chunk-based batch processing
* Optimized SQL queries
* Controlled DAG concurrency
* Reduced memory footprint

## ğŸ“ˆ Dashboard Capabilities

* Latest pipeline status
* Pipeline run history
* Data volume trends
* Data quality indicators
* Status distribution charts

## ğŸ“Œ Key Learnings

* Enterprise ETL architecture design
* Workflow orchestration using Airflow
* Data quality and governance
* Performance optimization techniques
* Production-level debugging and monitoring

## ğŸ“œLicense

This project is licensed under the MIT License. See `LICENSE`.
  
